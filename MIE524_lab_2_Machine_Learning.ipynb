{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MIE524 - Lab 2 - Machine Learning\n"
      ],
      "metadata": {
        "id": "jeW3xebFFzKp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup\n",
        "Let's setup Spark on the Colab environment."
      ],
      "metadata": {
        "id": "x03i53hhGTE5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqrJNAvvFTb9"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Authenticate a Google Drive client to download the files needed in the Spark job.\n"
      ],
      "metadata": {
        "id": "QdqKV5gWJjst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "metadata": {
        "id": "ToFm5dS0JkY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load packages."
      ],
      "metadata": {
        "id": "ihg3RpuCJ_vt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf"
      ],
      "metadata": {
        "id": "gIFbRszCJ-gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize Spark context."
      ],
      "metadata": {
        "id": "RLbVEvdaKBqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the session\n",
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
        "\n",
        "# create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "ZyhNZGaTKFFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data\n",
        "![MNIST](https://upload.wikimedia.org/wikipedia/commons/thumb/2/27/MnistExamples.png/220px-MnistExamples.png)\n",
        "\n",
        "We will be using the [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset throughout this lab. a large collection of handwritten digits that is widely used for training and testing in the field of machine learning.\n",
        "\n",
        "This loads the MNIST dataset in the LibSVM format, where each digit is represented as a sparse vector of grayscale pixel values."
      ],
      "metadata": {
        "id": "jn_fZxqJIV9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "id='1aJrdYMVmmnUKYhLTlXtyB0FQ9gYJqCrs'\n",
        "downloaded = drive.CreateFile({'id': id})\n",
        "downloaded.GetContentFile('mnist-digits-train.txt')\n",
        "\n",
        "id='1yLwxRaJIyrC03yxqbTKpedMmHEF86AAq'\n",
        "downloaded = drive.CreateFile({'id': id})\n",
        "downloaded.GetContentFile('mnist-digits-test.txt')"
      ],
      "metadata": {
        "id": "Z32o_fmnI6ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training = spark.read.format(\"libsvm\").option(\"numFeatures\",\"784\").load(\"mnist-digits-train.txt\")\n",
        "test = spark.read.format(\"libsvm\").option(\"numFeatures\",\"784\").load(\"mnist-digits-test.txt\")\n",
        "\n",
        "# Cache data for multiple uses\n",
        "training.cache()\n",
        "test.cache()"
      ],
      "metadata": {
        "id": "GxNlBxBLJJAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training.show(truncate=False)"
      ],
      "metadata": {
        "id": "gPXx0IkZKyZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training.printSchema()"
      ],
      "metadata": {
        "id": "3v5Inhb8Kycf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.printSchema()"
      ],
      "metadata": {
        "id": "fENQgn1AK2o8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## PART 1 - Random Forrest\n",
        "We will build a random forrest model from scratch."
      ],
      "metadata": {
        "id": "B7n41MhIG_tX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn import tree\n",
        "from sklearn.utils import resample\n",
        "\n",
        "from pyspark.ml.feature import StringIndexer, VectorIndexer\n"
      ],
      "metadata": {
        "id": "AiF7SaFQG7vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define parameters\n",
        "\n",
        "**n_estimators**: int - The number of classification trees that are used.\n",
        "\n",
        "**max_features**: int - The maximum number of features that the classification trees are allowed to use.\n",
        "\n",
        "**min_samples_split**: int - The minimum number of samples needed to make a split when building a tree.\n",
        "\n",
        "**min_gain**: float - The minimum impurity required to split the tree further.\n",
        "\n",
        "**max_depth**: int - The maximum depth of a tree.\n",
        "\n",
        "**n_samples**: int - The number of samples in each tree is max_samples * X.shape[0]."
      ],
      "metadata": {
        "id": "hesP2Ssz5LsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_estimators = 20\n",
        "max_features = 20\n",
        "min_samples_split = 2\n",
        "min_impurity_decrease = 0\n",
        "max_depth = None\n",
        "n_samples = 0.05\n",
        "\n",
        "n_features = len(training.select('features').first()[0])\n",
        "# If max_features have not been defined => select it as\n",
        "# sqrt(n_features)\n",
        "if not max_features:\n",
        "    max_features = int(math.sqrt(n_features))"
      ],
      "metadata": {
        "id": "rAwV9cP-5N6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the set of decision trees, we will use scikit's decision tree implementation"
      ],
      "metadata": {
        "id": "NS_J8JBD76zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trees = []\n",
        "for _ in range(n_estimators):\n",
        "    trees.append(\n",
        "        tree.DecisionTreeClassifier(\n",
        "            min_samples_split=min_samples_split,\n",
        "            min_impurity_decrease=min_impurity_decrease,\n",
        "            max_depth=max_depth))"
      ],
      "metadata": {
        "id": "yxjBfwUm8CB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create sample subsets for training for each tree."
      ],
      "metadata": {
        "id": "r91sMz8wUikj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sample subsets for each decision tree\n",
        "subsets = []\n",
        "for i in tqdm(range(n_estimators)):\n",
        "    subsets.append(training.sample(True, n_samples, 524))"
      ],
      "metadata": {
        "id": "fZ3AbZj8LuIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data to pandas dataframe to use with scikit\n",
        "subsets_pd = []\n",
        "for i in tqdm(range(n_estimators)):\n",
        "    X_subset = pd.DataFrame(subsets[i].toPandas().apply(lambda row: row['features'].toArray(), axis=1).tolist())\n",
        "    y_subset = subsets[i].toPandas()[['label']]\n",
        "    subsets_pd.append([X_subset, y_subset])"
      ],
      "metadata": {
        "id": "PaS_DQe8Dikc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the training loop (*fit* function)."
      ],
      "metadata": {
        "id": "D5I-YJ1RGVsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "for i in range(n_estimators):\n",
        "    X_subset = subsets_pd[i][0]\n",
        "    y_subset = subsets_pd[i][1]\n",
        "\n",
        "    # Feature bagging (select random subsets of the features)\n",
        "    idx = np.random.choice(range(n_features), size=max_features, replace=True)\n",
        "    # Save the indices of the features for prediction\n",
        "    trees[i].feature_indices = idx\n",
        "    # Choose the features corresponding to the indices\n",
        "    X_subset = X_subset.iloc[:, idx]\n",
        "    # Fit the tree to the data\n",
        "    trees[i].fit(X_subset, y_subset)"
      ],
      "metadata": {
        "id": "OlXRVHl6MtyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are ready to make some predictions."
      ],
      "metadata": {
        "id": "2QRl0CaFQux6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = pd.DataFrame(test.toPandas().apply(lambda row: row['features'].toArray(), axis=1).tolist())\n",
        "y_test = test.toPandas()[['label']]\n",
        "\n",
        "y_preds = np.empty((X_test.shape[0], len(trees)))\n",
        "# Let each tree make a prediction on the data\n",
        "for i, t in enumerate(trees):\n",
        "    # Indices of the features that the tree has trained on\n",
        "    idx = t.feature_indices\n",
        "    # Make a prediction based on those features\n",
        "    prediction = t.predict(X_test.iloc[:, idx])\n",
        "    y_preds[:, i] = prediction\n",
        "\n",
        "y_pred = []\n",
        "# For each sample\n",
        "for sample_predictions in y_preds:\n",
        "    # Select the most common class prediction\n",
        "    y_pred.append(np.bincount(sample_predictions.astype('int')).argmax())"
      ],
      "metadata": {
        "id": "67tPRfsfQ01x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Putting everything together..."
      ],
      "metadata": {
        "id": "HNUsXswqMQsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomForest():\n",
        "    \"\"\"Random Forest classifier. Uses a collection of classification trees that\n",
        "    trains on random subsets of the data using a random subsets of the features.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    n_estimators: int\n",
        "        The number of classification trees that are used.\n",
        "    max_features: int\n",
        "        The maximum number of features that the classification trees are allowed to\n",
        "        use.\n",
        "    min_samples_split: int\n",
        "        The minimum number of samples needed to make a split when building a tree.\n",
        "    min_impurity_decrease: float\n",
        "        A node will be split if this split induces a decrease of the impurity\n",
        "        greater than or equal to this value.\n",
        "    max_depth: int\n",
        "        The maximum depth of a tree.\n",
        "    n_samples: int\n",
        "        The number of samples in each tree is max_samples * X.shape[0].\n",
        "    \"\"\"\n",
        "    def __init__(self, n_estimators=100, max_features=None, min_samples_split=2,\n",
        "                 min_impurity_decrease=0, max_depth=float(\"inf\"), n_samples = 1):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_features = max_features\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_impurity_decrease = min_impurity_decrease\n",
        "        self.max_depth = max_depth\n",
        "        self.n_samples = n_samples\n",
        "\n",
        "        # Initialize decision trees\n",
        "        self.trees = []\n",
        "        for _ in range(n_estimators):\n",
        "            self.trees.append(\n",
        "                tree.DecisionTreeClassifier(\n",
        "                    min_samples_split=self.min_samples_split,\n",
        "                    min_impurity_decrease=min_impurity_decrease,\n",
        "                    max_depth=self.max_depth))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_features = np.shape(X)[1]\n",
        "        # If max_features have not been defined => select it as\n",
        "        # sqrt(n_features)\n",
        "        if not self.max_features:\n",
        "            self.max_features = int(math.sqrt(n_features))\n",
        "\n",
        "        # Create sample subsets\n",
        "        subsets = []\n",
        "        for i in tqdm(range(self.n_estimators)):\n",
        "            sample = pd.concat([X,y], axis = 1).sample(frac=self.n_samples, replace=True, random_state=524)\n",
        "            subsets.append([sample.drop('label', axis=1), sample[['label']]])\n",
        "\n",
        "\n",
        "        for i in range(self.n_estimators):\n",
        "            X_subset = subsets[i][0]\n",
        "            y_subset = subsets[i][1]\n",
        "\n",
        "            # Feature bagging (select random subsets of the features)\n",
        "            idx = np.random.choice(range(n_features), size=max_features, replace=True)\n",
        "            # Save the indices of the features for prediction\n",
        "            self.trees[i].feature_indices = idx\n",
        "            # Choose the features corresponding to the indices\n",
        "            X_subset = X_subset.iloc[:, idx]\n",
        "            # Fit the tree to the data\n",
        "            self.trees[i].fit(X_subset, y_subset)\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_preds = np.empty((X.shape[0], len(self.trees)))\n",
        "        # Let each tree make a prediction on the data\n",
        "        for i, tree in enumerate(self.trees):\n",
        "            # Indices of the features that the tree has trained on\n",
        "            idx = tree.feature_indices\n",
        "            # Make a prediction based on those features\n",
        "            prediction = tree.predict(X.iloc[:, idx])\n",
        "            y_preds[:, i] = prediction\n",
        "\n",
        "        y_pred = []\n",
        "        # For each sample\n",
        "        for sample_predictions in y_preds:\n",
        "            # Select the most common class prediction\n",
        "            y_pred.append(np.bincount(sample_predictions.astype('int')).argmax())\n",
        "        return y_pred"
      ],
      "metadata": {
        "id": "JaB0VoyxLYev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's train and predict using the RandomForest class."
      ],
      "metadata": {
        "id": "Qs8NITmrfBUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize classifier\n",
        "clf = RandomForest(n_estimators = 20, max_features = 20,\n",
        "                   min_samples_split = 2, min_impurity_decrease = 0,\n",
        "                   max_depth = None, n_samples = 0.05)"
      ],
      "metadata": {
        "id": "ouUf4JbUc7sA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data\n",
        "X_training = pd.DataFrame(training.toPandas().apply(lambda row: row['features'].toArray(), axis=1).tolist())\n",
        "y_training = training.toPandas()[['label']].astype(int)\n",
        "\n",
        "X_test = pd.DataFrame(test.toPandas().apply(lambda row: row['features'].toArray(), axis=1).tolist())\n",
        "y_test = test.toPandas()[['label']].astype(int)"
      ],
      "metadata": {
        "id": "wd7jW_9vfp5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit model to data\n",
        "clf.fit(X_training, y_training)"
      ],
      "metadata": {
        "id": "yHWj5Jgsdz1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = clf.predict(X_test)"
      ],
      "metadata": {
        "id": "dB0QWtUAg2nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate model performance"
      ],
      "metadata": {
        "id": "06FyBCUyg9zr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "p5l3GeD0hFQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disp = ConfusionMatrixDisplay.from_predictions(y_test, y_pred)\n",
        "disp.figure_.suptitle(\"Confusion Matrix\")\n",
        "print(f\"Confusion matrix:\\n{disp.confusion_matrix}\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r9F6iwVFqY3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare with scikit's implementation of random forest."
      ],
      "metadata": {
        "id": "Nu5zg1xlt7p4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "sk_clf = RandomForestClassifier(n_estimators = 20, max_features = 20,\n",
        "                   min_samples_split = 2, min_impurity_decrease = 0,\n",
        "                   max_depth = None, max_samples = 0.05)\n",
        "\n",
        "sk_clf.fit(X_training, y_training)\n",
        "\n",
        "y_pred_sk = sk_clf.predict(X_test)"
      ],
      "metadata": {
        "id": "7JuVVDOyq5Ne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_pred_sk))\n"
      ],
      "metadata": {
        "id": "wuYshJJyulmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see MLlib's performance, which can fully leverage the datatype of libsvm."
      ],
      "metadata": {
        "id": "mSYbLwHXu6m8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Index labels, adding metadata to the label column.\n",
        "# Fit on whole dataset to include all labels in index.\n",
        "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(training.union(test))\n",
        "\n",
        "# Automatically identify categorical features, and index them.\n",
        "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
        "featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\").fit(training.union(test))\n",
        "\n",
        "\n",
        "# Train a RandomForest model.\n",
        "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=20)\n",
        "\n",
        "# Convert indexed labels back to original labels.\n",
        "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
        "                               labels=labelIndexer.labels)\n",
        "\n",
        "# Chain indexers and forest in a Pipeline\n",
        "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])\n",
        "\n",
        "# Train model.  This also runs the indexers.\n",
        "model = pipeline.fit(training)\n",
        "\n",
        "# Make predictions.\n",
        "predictions = model.transform(test)\n",
        "\n",
        "# Select example rows to display.\n",
        "predictions.show(5)\n"
      ],
      "metadata": {
        "id": "nmCS9LrSuu10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predictions.rdd.map(lambda x: x.prediction).collect()\n",
        "\n",
        "y_pred_spark = predictions.rdd.map(lambda x: x.prediction).collect()\n",
        "y_test_spark = predictions.rdd.map(lambda x: x.indexedLabel).collect()"
      ],
      "metadata": {
        "id": "3F1QeV9C7f4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test_spark, y_pred_spark))\n"
      ],
      "metadata": {
        "id": "lkJ9-exuxFlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PART 2 - Deep Learning\n",
        "We will build a simple feed forwarnd neural network in this part.\n",
        "\n",
        "![FFW](https://images.deepai.org/django-summernote/2019-06-06/5c17d9c2-0ad4-474c-be8d-d6ae9b094e74.png)"
      ],
      "metadata": {
        "id": "v72weLXXAELM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "z5wx9J-QxVi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Torch Introduction"
      ],
      "metadata": {
        "id": "w0H5ewB94rms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializing a tensor"
      ],
      "metadata": {
        "id": "1ynaDSIj4ye9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [[1, 2],[3, 4]]\n",
        "x_data = torch.tensor(data)\n",
        "\n",
        "print(x_data)"
      ],
      "metadata": {
        "id": "jgIFCcQK4rG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize a tensor with random values"
      ],
      "metadata": {
        "id": "rkxCfzHm5D1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
        "print(x_rand)"
      ],
      "metadata": {
        "id": "LvWDI8-_5DTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attributes of a tensor"
      ],
      "metadata": {
        "id": "ssRZEcAj5QB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = torch.rand(3,4)\n",
        "\n",
        "print(f\"Shape of tensor: {tensor.shape}\")\n",
        "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
        "print(f\"Device tensor is stored on: {tensor.device}\")\n",
        "print(tensor)"
      ],
      "metadata": {
        "id": "h7PcJhi-5R8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standard operations on tensors"
      ],
      "metadata": {
        "id": "CHXuWq2S5tFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"First row: {tensor[0]}\")\n",
        "print(f\"First column: {tensor[:, 0]}\")\n",
        "print(f\"Last column: {tensor[..., -1]}\")"
      ],
      "metadata": {
        "id": "jEyOGBb55eKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor[:,1] = 0\n",
        "print(tensor)"
      ],
      "metadata": {
        "id": "6FW1A3rJ5jg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Arithmetic operations on tensors"
      ],
      "metadata": {
        "id": "OpUCkN9K6Cxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# matrix multiplication of two tensors, y1 and y2 are equivalent\n",
        "\n",
        "y1 = tensor @ tensor.T\n",
        "y2 = tensor.matmul(tensor.T)\n"
      ],
      "metadata": {
        "id": "5824P6d36Fnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# element-wise product of the elemnts of the two tensors\n",
        "\n",
        "z1 = tensor * tensor\n",
        "z2 = tensor.mul(tensor)"
      ],
      "metadata": {
        "id": "vVaRlvDM6QiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare data"
      ],
      "metadata": {
        "id": "F34Bx1H-GX4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset\n",
        "training_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "test_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")"
      ],
      "metadata": {
        "id": "q4h3mU8ZGZWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataloader - used to perform mini batch or stochastic gradient descent by acting as an iterable.\n",
        "batch_size = 100\n",
        "\n",
        "train_loader = DataLoader(dataset=training_data,shuffle=True,batch_size=batch_size)\n",
        "test_loader = DataLoader(dataset=test_data,shuffle=True,batch_size=batch_size)"
      ],
      "metadata": {
        "id": "VJ2kjPC_GpD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for X, y in test_loader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break"
      ],
      "metadata": {
        "id": "l9eaNqCp4CGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples = iter(test_loader)\n",
        "example_data, example_targets = next(examples)\n",
        "for i in range(6):\n",
        "    plt.subplot(2,3,i+1)\n",
        "    plt.imshow(example_data[i][0], cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dgf8L9n47b7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the neural network"
      ],
      "metadata": {
        "id": "MIu5TjP_H9HO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`super()` returns a temporary object of the superclass that then allows you to call that superclassâ€™s methods."
      ],
      "metadata": {
        "id": "X9Cac_z_8RQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a neural network with 2 linear hidden layers with a Relu activation function\n",
        "from torch import nn\n",
        "class net(nn.Module):\n",
        "    def __init__(self,input_size,output_size, hidden_size):\n",
        "        super(net,self).__init__()\n",
        "        self.l1 = nn.Linear(input_size,hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.l2 = nn.Linear(hidden_size,output_size)\n",
        "    def forward(self,x):\n",
        "        output = self.l1(x)\n",
        "        output = self.relu(output)\n",
        "        output = self.l2(output)\n",
        "        return output\n",
        "\n",
        "input_size = 28 * 28\n",
        "output_size = 10\n",
        "hidden_size = 500\n",
        "\n",
        "model = net(input_size, output_size, hidden_size)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "358JkVF_Hdi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define loss function"
      ],
      "metadata": {
        "id": "2Ysek0i1KMeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "FlIrsCidI2Kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the optimizer (specify optimization method, learning rate, etc.)"
      ],
      "metadata": {
        "id": "XToLLqqQLGiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "5IXr1N16LFyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training loop"
      ],
      "metadata": {
        "id": "SDxEktnkLY8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 2\n",
        "n_total_steps = len(train_loader)\n",
        "\n",
        "lossval = []\n",
        "for j in range(num_epochs):\n",
        "    for i, (x_train, y_train) in enumerate(train_loader):\n",
        "        #prediction\n",
        "        y_pred = model(x_train.reshape(-1, 28*28))\n",
        "\n",
        "        #calculating loss\n",
        "        loss = criterion(y_pred,y_train.reshape(-1))\n",
        "\n",
        "        #calculating accuracy\n",
        "        correct = (y_pred.argmax(1) == y_train).type(torch.float).sum().item()\n",
        "        accuracy_batch = correct/batch_size\n",
        "\n",
        "        #backprop\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        #print batch information\n",
        "        if (i+1) % 100 == 0:\n",
        "            print (f'Epoch [{j+1}/{num_epochs}], Step[{i+1}/{n_total_steps}], Train Accuracy: {accuracy_batch:.2f}%, Loss: {loss.item():.4f}')\n",
        "            lossval.append(loss.item())\n"
      ],
      "metadata": {
        "id": "jRp_fa0_LXis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(lossval)"
      ],
      "metadata": {
        "id": "4eUfBC96LtG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save and load a model"
      ],
      "metadata": {
        "id": "lXG2pP3YaF79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'model_nn.pth')"
      ],
      "metadata": {
        "id": "NoPZqfpqaItz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 28 * 28\n",
        "output_size = 10\n",
        "hidden_size = 500\n",
        "\n",
        "model = net(input_size, output_size, hidden_size)\n",
        "model.load_state_dict(torch.load('model_nn.pth'))\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "Q5TjBj8OabWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predict"
      ],
      "metadata": {
        "id": "3b6VHnVsZVa_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    y_test_nn = []\n",
        "    y_pred_nn = []\n",
        "    for x_test, y_test in test_loader:\n",
        "        outputs = model(x_test.reshape(-1, 28*28))\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        y_pred_nn += predicted.tolist()\n",
        "        y_test_nn += y_test.tolist()"
      ],
      "metadata": {
        "id": "qOa7qNf2YEZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
        "\n",
        "print(classification_report(y_test_nn, y_pred_nn))"
      ],
      "metadata": {
        "id": "RaE3Tk_2KEnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-aG4HCc4xJuj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
